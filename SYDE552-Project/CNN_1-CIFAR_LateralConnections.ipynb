{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"H3WMlQFfDxB2"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torchvision import models, datasets, transforms\n","from torch.utils.data import DataLoader, Dataset, Subset, random_split\n","from PIL import Image\n","from torchvision.datasets import ImageFolder\n","from tqdm import tqdm\n","import numpy as np\n","from torchvision.models import resnet50\n","from torch.autograd import Variable\n","import os\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","import torchvision\n","import torchvision.transforms as transforms\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","source":["# Check if GPU is available and set device accordingly\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"],"metadata":{"id":"QNCGRHEz4y78"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define transformations for training dataset (with data augmentation)\n","transform = transforms.Compose([\n","    #transforms.RandomRotation(15),  # Rotation by Â±15 degrees\n","    #transforms.RandomHorizontalFlip(),  # Horizontal flip\n","    #transforms.RandomVerticalFlip(),  # Uncomment if vertical flip is desired\n","    transforms.ToTensor(),  # Convert images to tensor\n","    #transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize the images\n","])\n","\n","# Load CIFAR-10 dataset\n","trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n","trainloader = DataLoader(trainset, batch_size=32, shuffle=True, num_workers=2)\n","\n","testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n","testloader = DataLoader(testset, batch_size=32, shuffle=False, num_workers=2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3wVlnIL9D5xn","executionInfo":{"status":"ok","timestamp":1713239952658,"user_tz":240,"elapsed":2514,"user":{"displayName":"A P","userId":"03103991736051634190"}},"outputId":"18a91653-8a1d-4ac4-dcd0-17b3777100b8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Files already downloaded and verified\n","Files already downloaded and verified\n"]}]},{"cell_type":"code","source":["class CNNLateralConnections(nn.Module):\n","    def __init__(self, dropout_rate=0.5, num_classes=10):\n","        super(CNNLateralConnections, self).__init__()\n","        self.conv1 = nn.Conv2d(in_channels=3, out_channels=96, kernel_size=5, stride=1, padding=2)  # Added padding to maintain size\n","        self.bn1 = nn.BatchNorm2d(num_features=96)\n","        self.conv2 = nn.Conv2d(in_channels=96, out_channels=256, kernel_size=5, stride=1, padding=2)  # Added padding to maintain size\n","        self.bn2 = nn.BatchNorm2d(num_features=256)\n","        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n","        self.dropout1 = nn.Dropout(p=dropout_rate)\n","        self.conv3 = nn.Conv2d(in_channels=352, out_channels=384, kernel_size=3, stride=1, padding=1)  # Input channels must match the sum from concatenated layers\n","        self.conv4 = nn.Conv2d(in_channels=384, out_channels=384, kernel_size=3, stride=1, padding=1)\n","        self.conv5 = nn.Conv2d(in_channels=1120, out_channels=256, kernel_size=3, stride=1, padding=1)  # Adjusted to accommodate all concatenated layers\n","        self.dropout2 = nn.Dropout(p=dropout_rate)\n","        self.fc1 = nn.Linear(in_features=65536, out_features=num_classes)  # Adjusted size based on flattened dimensions from final conv layer\n","\n","    def forward(self, x):\n","        x1 = F.relu(self.bn1(self.conv1(x)))  # Output size: [96, 224, 224]\n","        x2 = F.relu(self.bn2(self.conv2(x1)))  # Output size: [256, 224, 224]\n","        x2_pooled = self.pool1(x2)  # Output size: [256, 112, 112]\n","\n","        x1_resized = F.adaptive_avg_pool2d(x1, x2_pooled.shape[2:])  # Resize x1 to match x2_pooled dimensions\n","        x3_input = torch.cat([x1_resized, x2_pooled], dim=1)  # Channel dimension: 96 + 256 = 352\n","        x3 = F.relu(self.conv3(x3_input))  # Output size: [384, 112, 112]\n","        x4 = F.relu(self.conv4(x3))  # Output size: [384, 112, 112]\n","\n","        x2_resized = F.adaptive_avg_pool2d(x2, x4.shape[2:])  # Resize x2 to match x4 dimensions\n","        x5_input = torch.cat([x4, x1_resized, x2_resized, x4], dim=1)  # Channel dimension: 384 + 96 + 256 + 384 = 1120\n","        x5 = F.relu(self.conv5(x5_input))  # Output size: [256, 112, 112]\n","        x5 = x5.view(x5.size(0), -1)  # Flatten the tensor\n","\n","        x5 = self.dropout2(x5)  # Apply dropout before the fully connected layer\n","        #print(x5.size())\n","        output = self.fc1(x5)  # Final output\n","        return output"],"metadata":{"id":"NEWOcbi6F1FY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Training loop\n","def train(epoch, net, trainloader, optimizer, criterion):\n","    net.train()\n","    running_loss = 0.0\n","    correct = 0\n","    total = 0\n","    for i, data in enumerate(trainloader, 0):\n","        inputs, labels = data[0].to(device), data[1].to(device)  # Move data to GPU\n","        optimizer.zero_grad()\n","        outputs = net(inputs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","        running_loss += loss.item()\n","\n","        # Compute accuracy\n","        _, predicted = torch.max(outputs.data, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","\n","        if i % 2000 == 1999:  # Print every 2000 mini-batches\n","            print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 2000))\n","            running_loss = 0.0\n","\n","    # Calculate accuracy after each epoch\n","    accuracy = 100 * correct / total\n","    epoch_list.append(epoch + 1)  # Append epoch number to list\n","    accuracy_list.append(accuracy)  # Append accuracy to list\n","    print('Epoch [%d], Accuracy on training images: %d %%' % (epoch + 1, accuracy))"],"metadata":{"id":"g4fsHd2zFM2G"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Test the model\n","def test(net, testloader):\n","    net.eval()\n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","        for data in testloader:\n","            images, labels = data[0].to(device), data[1].to(device)  # Move data to GPU\n","            outputs = net(images)\n","            _, predicted = torch.max(outputs.data, 1)\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","    print('Accuracy on test images: %d %%' % (100 * correct / total))"],"metadata":{"id":"DWOPg_c-4rgt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Iniltialize the model\n","net = CNNLateralConnections()\n","net.to(device)\n","\n","#Initialize optimizer and Loss function\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"],"metadata":{"id":"WS_gW6cHESKL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Lists to store epoch and accuracy values\n","epoch_list = []\n","accuracy_list = []\n","# Train the model\n","for epoch in range(10):  # loop over the dataset multiple times\n","    train(epoch, net, trainloader, optimizer, criterion)\n","\n","# Test the model\n","test(net, testloader)"],"metadata":{"id":"noqs94pLK2nF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1713240636085,"user_tz":240,"elapsed":318119,"user":{"displayName":"A P","userId":"03103991736051634190"}},"outputId":"e2323f4f-6ef7-4817-f0e8-25e00ea99bd7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch [1], Accuracy on training images: 48 %\n","Epoch [2], Accuracy on training images: 63 %\n","Epoch [3], Accuracy on training images: 69 %\n","Epoch [4], Accuracy on training images: 73 %\n","Epoch [5], Accuracy on training images: 76 %\n","Epoch [6], Accuracy on training images: 79 %\n","Epoch [7], Accuracy on training images: 81 %\n","Epoch [8], Accuracy on training images: 83 %\n","Epoch [9], Accuracy on training images: 85 %\n","Epoch [10], Accuracy on training images: 87 %\n","Accuracy on test images: 77 %\n"]}]},{"cell_type":"code","source":["# Plot epoch vs accuracy\n","plt.plot(epoch_list, accuracy_list, marker='o')\n","plt.title('Epoch vs Accuracy')\n","plt.xlabel('Epoch')\n","plt.ylabel('Accuracy (%)')\n","plt.grid(True)\n","plt.show()"],"metadata":{"id":"zk5IyTM-d1jZ"},"execution_count":null,"outputs":[]}]}